{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import spacy\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spacy.training.example import Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = os.path.abspath(\"../../data/datasets/ner-swiss-receipts.json\")  # Path to your training data file\n",
    "OUTPUT_DIR = os.path.abspath(\"../models/receipts-ner-v02\")  # TODO: make sure to use a new version\n",
    "ITERATIONS = 30  # Number of training iterations\n",
    "BATCH_SIZE = 8  # Batch size for training\n",
    "DROPOUT = 0.5  # Dropout rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as file:\n",
    "    dataset = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reformat for Spacy & Remove unwanted Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for entry in dataset:\n",
    "    text = entry[\"text\"]\n",
    "    entities = [entity for entity in entry[\"entities\"] if entity[2] == 'RECEIPT_ITEM']\n",
    "    data.append((text, {\"entities\": entities}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Training Data:\n",
      "-----------------------------------------\n",
      "HAMNUGLU GmbH\n",
      "HERTENSTEIN\n",
      "\n",
      "Hertensteinstrasse 32\n",
      "\n",
      "6004 Luzern\n",
      "\n",
      "Tel 041 419 71 51\n",
      "RER\n",
      "\n",
      "#CS0 Rechnungsnummer: 36 POSEJ Order: el\n",
      "KASSE 32- 2071272024 14:23:55\n",
      "\n",
      "ANZ ARTIKEL GESAMT \"\n",
      "1MeChicken 6.90 M\n",
      "ı MM 9 McNuggets f 15.800. Ep\n",
      "19 Nucpets 2\n",
      "1 Cocktail Sauce\n",
      "\n",
      ". 1 Potatoes Sauce\n",
      "1 Hen ohne Ksäure 0.5 2\n",
      "1 Medium Pommes Frite ed\n",
      "1 Cocktail Sauce en\n",
      "\n",
      "Subtotal 2.10 u\n",
      "Discount: i -1.30 Be\n",
      "INNEN TOTAL 21.40 wo\n",
      "VISA 210 0\n",
      "\n",
      "st.Nr. CHE - 449.055.085 MWST ] 0\n",
      "SATZ BRUTTO MWST.\n",
      "incl. MWST  8.10 21.40 1.60\n",
      "\n",
      " Kundenbeleg\n",
      "\n",
      "Buchung\n",
      ". Visa payWave contact less\n",
      "\n",
      "4323 22XX XXXX 1361\n",
      "\n",
      "20.12.2024 14:23:41\n",
      "u Trm-Id: 10138349\n",
      "AID: 40000000031010\n",
      "Irx. Seg-Cnt: Mo\n",
      "\n",
      "..... Trx. Ref-No: .\n",
      "\n",
      "Auth, Code:\n",
      "\n",
      "-----------------------------------------\n",
      "Entity-Type: RECEIPT_ITEM, Entity text: 1MeChicken 6.90 M\n",
      "Entity-Type: RECEIPT_ITEM, Entity text: ı MM 9 McNuggets f 15.800. Ep\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample Training Data:\")\n",
    "sample_text, sample_annotaiton = train[0]\n",
    "print(\"-----------------------------------------\")\n",
    "print(sample_text)\n",
    "print(\"-----------------------------------------\")\n",
    "for ent in sample_annotaiton.get(\"entities\"):\n",
    "    print(f'Entity-Type: {ent[2]}, Entity text: {sample_text[ent[0]:ent[1]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load an emtpy spacy model for specific language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a blank model, or replace with spacy.load(\"en_core_web_sm\")\n",
    "nlp = spacy.blank(\"de\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Named-Entity Recognition Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.add_pipe(\"ner\", last=True)\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Labels to Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, annotations in train:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    Example.from_dict(nlp.make_doc(text), annotations) for text, annotations in train\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/30, Loss: 5313.0225\n",
      "Epoch 2/30, Loss: 1999.4539\n",
      "Epoch 3/30, Loss: 1437.6208\n",
      "Epoch 4/30, Loss: 1399.8524\n",
      "Epoch 5/30, Loss: 1038.3199\n",
      "Epoch 6/30, Loss: 769.4518\n",
      "Epoch 7/30, Loss: 723.7105\n",
      "Epoch 8/30, Loss: 651.6862\n",
      "Epoch 9/30, Loss: 557.2901\n",
      "Epoch 10/30, Loss: 464.8676\n",
      "Epoch 11/30, Loss: 406.5923\n",
      "Epoch 12/30, Loss: 352.5955\n",
      "Epoch 13/30, Loss: 326.8011\n",
      "Epoch 14/30, Loss: 249.9826\n",
      "Epoch 15/30, Loss: 251.2966\n",
      "Epoch 16/30, Loss: 206.3507\n",
      "Epoch 17/30, Loss: 217.6507\n",
      "Epoch 18/30, Loss: 181.3991\n",
      "Epoch 19/30, Loss: 183.8387\n",
      "Epoch 20/30, Loss: 173.1094\n",
      "Epoch 21/30, Loss: 146.1970\n",
      "Epoch 22/30, Loss: 160.8596\n",
      "Epoch 23/30, Loss: 140.8577\n",
      "Epoch 24/30, Loss: 128.4356\n",
      "Epoch 25/30, Loss: 122.5660\n",
      "Epoch 26/30, Loss: 107.2497\n",
      "Epoch 27/30, Loss: 130.2041\n",
      "Epoch 28/30, Loss: 108.4909\n",
      "Epoch 29/30, Loss: 95.8912\n",
      "Epoch 30/30, Loss: 96.8432\n"
     ]
    }
   ],
   "source": [
    "# Disable other pipes during training for efficiency\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    optimizer = nlp.begin_training()\n",
    "        \n",
    "    # Training loop\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(ITERATIONS):\n",
    "        random.shuffle(examples)\n",
    "        losses = {}\n",
    "        for batch in spacy.util.minibatch(examples, size=BATCH_SIZE):\n",
    "            nlp.update(batch, drop=DROPOUT, losses=losses)\n",
    "        print(f\"Epoch {epoch + 1}/{ITERATIONS}, Loss: {losses['ner']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to /Users/timon/git/dspro1-receipt-ocr/named-entity-recognition/models/receipts-ner-v02...\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "print(f\"Saving model to {OUTPUT_DIR}...\")\n",
    "nlp.to_disk(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(matrix, labels):\n",
    "    \"\"\"\n",
    "    Prints a confusion matrix as a formatted string.\n",
    "\n",
    "    Args:\n",
    "        matrix (list of list of int): Confusion matrix as a 2D list.\n",
    "        labels (list of str): Labels for the matrix.\n",
    "    \"\"\"\n",
    "    max_label_length = max(len(label) for label in labels)\n",
    "    padding = max_label_length + 2\n",
    "\n",
    "    # Print header\n",
    "    header = \" \" * padding + \" \".join(f\"{label:>{padding}}\" for label in labels)\n",
    "    print(f'{\"\":<15} Predicted Values')\n",
    "    print(header)\n",
    "\n",
    "    # Print each row\n",
    "    for label, row in zip(labels, matrix):\n",
    "        row_str = \" \".join(f\"{cell:>{padding}}\" for cell in row)\n",
    "        print(f\"{label:>{padding}} {row_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                Predicted Values\n",
      "                Entity   No Entity\n",
      "     Entity          26           5\n",
      "  No Entity           7           0\n",
      "\n",
      "Performance Measures:\n",
      "Precision: 0.7878787878787878\n",
      "Recall: 0.8387096774193549\n",
      "F1 Score: 0.8125\n"
     ]
    }
   ],
   "source": [
    "true_positive = []\n",
    "false_positive = []\n",
    "false_negative = []\n",
    "\n",
    "index = 0\n",
    "\n",
    "for text, annotations in test:\n",
    "    # Process the text\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Prepare expected values (y)\n",
    "    expected_entities = []\n",
    "    for ent in annotations.get(\"entities\"):    \n",
    "        if ent[2] == 'RECEIPT_ITEM':\n",
    "            expected_entities.append(text[ent[0]:ent[1]])\n",
    "\n",
    "    # Print recognized entities\n",
    "    for ent in doc.ents:\n",
    "        if ent.text in expected_entities:\n",
    "            true_positive.append(ent.text)\n",
    "            expected_entities.remove(ent.text)\n",
    "        else:\n",
    "            false_positive.append(ent.text)\n",
    "            \n",
    "    false_negative.extend(expected_entities)\n",
    "    \n",
    "# Example usage\n",
    "labels = [\"Entity\", \"No Entity\"]\n",
    "confusion_matrix = [\n",
    "    [len(true_positive), len(false_negative)],\n",
    "    [len(false_positive), '0'],\n",
    "]\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "print_confusion_matrix(confusion_matrix, labels)\n",
    "\n",
    "print('\\nPerformance Measures:')\n",
    "precision = len(true_positive) / (len(true_positive) + len(false_positive))\n",
    "recall = len(true_positive) / (len(true_positive) + len(false_negative))\n",
    "f1_score = 2 * precision * recall / (precision + recall)\n",
    "print('Precision: ' + str(precision))\n",
    "print('Recall: ' + str(recall))\n",
    "print('F1 Score: ' + str(f1_score))\n",
    "\n",
    "# True-Positive = correct idetified entities\n",
    "# false_positive = actual entity that was not identified\n",
    "# False-negative = identified entity that is actually not a entity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
